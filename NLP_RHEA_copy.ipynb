{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9232153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rheaanand/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rheaanand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import gzip\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader as api\n",
    "import gensim.models\n",
    "from sklearn.linear_model import Perceptron\n",
    "import torch\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b91722",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e00ee963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rheaanand/miniconda3/envs/nlp/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/rheaanand/miniconda3/envs/nlp/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "text_df = pd.read_csv('data.tsv', error_bad_lines = False, sep = '\\t', warn_bad_lines = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e849b44",
   "metadata": {},
   "source": [
    "# Keeping Required Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fc07aeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********* Five sample reviews *********\n",
      "\n",
      "         star_rating                                        review_body\n",
      "2888589          5.0  Exceeded expectations.  The steel looks good, ...\n",
      "501710           3.0  I received the plastic cone yesterday and I ha...\n",
      "80789            3.0  Heating the hot dogs is fine, but the buns get...\n",
      "3014624          5.0  I grew up watching my mom use Pyrex cup measur...\n",
      "262012           4.0                      Looks great--giving as a gift\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "text_df = text_df[['star_rating','review_body']]\n",
    "\n",
    "text_df.dropna(subset = ['review_body'], inplace = True)\n",
    "text_df.dropna(subset = ['star_rating'], inplace = True)\n",
    "\n",
    "print(\"\\n********* Five sample reviews *********\\n\")\n",
    "print(text_df.sample(n=5, random_state=111))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186bef6",
   "metadata": {},
   "source": [
    "# Ramdomly Sampling 50000 reviews of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b76aa7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "<bound method NDFrame.head of          star_rating                                        review_body\n",
      "3377745          1.0                                  They both cracked\n",
      "137462           1.0  Leaves half the juice in the pulp. You can lit...\n",
      "3074771          1.0  I kid you not, this coffee grinder worked for ...\n",
      "2007652          1.0  Disappointed upon arrival~~<br />The caddy in ...\n",
      "4039144          1.0  Vacuumed 8 bags and died.  But hey it worked g...\n",
      "...              ...                                                ...\n",
      "314391           5.0  It's become a favorite pan to use. I use it fo...\n",
      "1119712          5.0  those knife are so good quality and so sharp.....\n",
      "2067376          5.0      Very nice,  ) ike that it comes with a stand.\n",
      "4768391          5.0  Heats water very quickly to boiling. Quiet.  A...\n",
      "2788460          5.0  Nice sturdy pan and nice size for cooking seve...\n",
      "\n",
      "[250000 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "#randomly sampling 50000 reviews of each rating \n",
    "r1 = text_df[text_df['star_rating'] == 1].sample(50000, random_state=89)\n",
    "r2 = text_df[text_df['star_rating'] == 2].sample(50000, random_state=89)\n",
    "r3 = text_df[text_df['star_rating'] == 3].sample(50000, random_state=89)\n",
    "r4 = text_df[text_df['star_rating'] == 4].sample(50000, random_state=89)\n",
    "r5 = text_df[text_df['star_rating'] == 5].sample(50000, random_state=89)\n",
    "\n",
    "\n",
    "list_text_df = [r1, r2, r3, r4, r5] \n",
    "text_df = pd.concat(list_text_df)\n",
    "print(text_df.size)\n",
    "print(text_df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26914ea",
   "metadata": {},
   "source": [
    "# Adding Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "dad6f7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 3377745    2\n",
       "137462     2\n",
       "3074771    2\n",
       "2007652    2\n",
       "4039144    2\n",
       "          ..\n",
       "314391     1\n",
       "1119712    1\n",
       "2067376    1\n",
       "4768391    1\n",
       "2788460    1\n",
       "Name: label, Length: 250000, dtype: int64>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################\n",
    "#adding new column label which gives 1 for > 3 rating 2 for < 3 rating and 3 for others (i.e ==3 rating)  \n",
    "text_df['label'] = np.where(text_df[\"star_rating\"] > 3, 1, np.where(text_df[\"star_rating\"] < 3, 2, 3))\n",
    "text_df['label'].head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6298f90b",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0cc12bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rheaanand/miniconda3/envs/nlp/lib/python3.9/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"http://www.amazon.com/10-5-round-stainless-steel-skimmer/dp/b00a6h272g/ref=sr_1_1?s=home-garden&ie=utf8&qid=1424472835&sr=1-1&keywords=11+3%2f4%22+oil+skimmer\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/rheaanand/miniconda3/envs/nlp/lib/python3.9/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.facebook.com/cherischocolates\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# DATA CLEANING \n",
    "###############\n",
    "\n",
    "# convert everything to lower case\n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].str.lower()     \n",
    "\n",
    "# Removing html tags using beautiful soup like <br> tags\n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: BeautifulSoup(str(x)).get_text()) \n",
    "\n",
    "# Removing urls from reviews\n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: re.sub(r'\\s*(https?://|www\\.)+\\S+(\\s+|$)', \" \", str(x), flags=re.UNICODE))\n",
    "\n",
    "# Removing Digits from the review_body \n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: re.sub(r\"[^\\D']+\", \" \", str(x), flags=re.UNICODE)) # remove all numbers\n",
    "\n",
    "# Removing Special Characters\n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: re.sub(r\"[^\\w']+\", \" \", str(x), flags=re.UNICODE)) # remove all special characters\n",
    "\n",
    "#remove more than one spaces\n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: re.sub(r'\\s+',' ', str(x), flags = re.UNICODE))  \n",
    "\n",
    "def contractionfunction(s):                      \n",
    "    s = s.apply(lambda x: contractions.fix(x))\n",
    "    return s\n",
    "\n",
    "text_df_onecol = contractionfunction(text_df[\"review_body\"])\n",
    "text_df[\"review_body\"] = text_df_onecol\n",
    "\n",
    "# convert everything to lower case again because contractions adds I \n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].str.lower()     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38946604",
   "metadata": {},
   "source": [
    "# Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c03ae22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# PRE-PROCESSING\n",
    "################\n",
    "\n",
    "# REMOVING STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# storing all the stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# remove stop words from each review  \n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: \" \".join([item  for item in str(x).split() if item not in stop_words]))\n",
    "\n",
    "# PERFORMING LEMMATIZATION\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(item)  for item in str(x).split()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234fa440",
   "metadata": {},
   "source": [
    "# Gensim word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a63ee870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "model = gensim.models.Word2Vec.load(\"./saved_models/custom_word2Vec.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81755df4",
   "metadata": {},
   "source": [
    "### Finding Similarity scores using  Google Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cd358a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pasta and sauce 0.5803348\n",
      "spoon and fork 0.4511615\n",
      "king + female - male = [('queen', 0.666067361831665)]\n",
      "spoon + prong - scoop = [('prongs', 0.5797024369239807)]\n"
     ]
    }
   ],
   "source": [
    "# find similarities using google news word2vec model\n",
    "print(\"pasta and sauce\", wv.similarity('pasta', 'sauce'))\n",
    "print(\"spoon and fork\", wv.similarity('spoon', 'fork'))\n",
    "\n",
    "# trying vector computations using word2vec\n",
    "\n",
    "print(\"king + female - male =\", wv.most_similar(positive=['king','female'],negative=['male'],topn=1))\n",
    "print(\"spoon + prong - scoop =\", wv.most_similar(positive=['spoon','prong'],negative=['bowl'],topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1078d0",
   "metadata": {},
   "source": [
    "### Training Custom Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c15418cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training word2vec model on the review data\n",
    "import gensim\n",
    "review_text = text_df['review_body'].apply(lambda x:[item for item in str(x).split()])\n",
    "model = gensim.models.Word2Vec(review_text, min_count=10, vector_size =300, window =11)\n",
    "model.save('./saved_models/custom_word2vec.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dfb427",
   "metadata": {},
   "source": [
    "### Finding Similarity scores using custom trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f1068e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pasta and sauce 0.4863852\n",
      "spoon and fork 0.68879133\n",
      "king + female - male = [('arthur', 0.5612844228744507)]\n",
      "spoon + prong - bowl = [('fork', 0.5765189528465271)]\n"
     ]
    }
   ],
   "source": [
    "# find similatities using custom word2vec trained model on review text\n",
    "print(\"pasta and sauce\", model.wv.similarity('pasta','sauce'))\n",
    "print(\"spoon and fork\", model.wv.similarity('spoon','fork'))\n",
    "\n",
    "# trying vector computations using word2vec custom trained model\n",
    "print(\"king + female - male =\", model.wv.most_similar(positive=['king','female'],negative=['male'],topn=1))\n",
    "print(\"spoon + prong - bowl =\", model.wv.most_similar(positive=['spoon','prong'],negative=['bowl'],topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593b640",
   "metadata": {},
   "source": [
    "# Analysing the similarity scores and vector computation results\n",
    "\n",
    "The similarity scores of the google word2vec model and custom word2vec model \n",
    "behave as anticipated, the google word2vec model gives higher accuracy for words that are more likely to be seen together in context in news data \n",
    "for eg. \n",
    "\n",
    "i) Pasta and Sauce have a higher similarity score in the google trained model than the custom model's predicted similarity, because our custom trained model is of kitchen products review, which is lesser likely to have pasta and sauce occuring in the same context in comparison to the Googles news dataset.\n",
    "\n",
    "ii) Spoon and Fork have a higher similarity score in the custom trained model in contrast to the google model's predicted similarity. This is because spoon and fork are more likely to appear in the same context more often in the custom trained word3vec model when compared to the google new trained model\n",
    "\n",
    "iii) The most commonly used word vector calculation to explain word2 vec:  King + Female - Male = Queen works as expected in the google trained model because these words will appear in same context and their meaning is repesented well in this model. However the custom trained model fails to achieve the axpected results because King and Queen words may not have been present enough number of times in the same context or could have conveyed a different meaning (for example: king size) in the custom training data based on amazon kitchen items review.\n",
    "\n",
    "iv) Another experiment at giving a fair chance to the custom trained model is to try the word vector calculation Spoon + Prongs - Bowl = Fork. This answer comes perfectly in the custom trained dataset because it is trained on kitchen products reviews and spoon fork prongs would appear in correct context. However the google news model fails to give the same result because these words will not appear as many times in the news text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2b7de5",
   "metadata": {},
   "source": [
    "# Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3daf026",
   "metadata": {},
   "source": [
    "### Finding Vector Representations for each review in the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "538722ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# creating a new column for the 300 size vectors obtained from google model and custom model\n",
    "# if word exists in the model obtain the vector from the respective model\n",
    "# otherwise consider the 300 size vector to be a [0] * 300 vector , i.e a vector unaffecting the vector sum\n",
    "# if review is empty, maybe after removing stop words, vector representation of such reviews are [0]*300\n",
    "\n",
    "text_df[\"word2vec_google\"] = text_df['review_body'].apply(lambda x:(sum([np.array(wv[item]) if item in wv else np.array([0]*300) for item in str(x).split()])/len(str(x).split()) if len(str(x).split())>0 else np.array([0]*300)))\n",
    "text_df[\"word2vec_custom\"] = text_df['review_body'].apply(lambda x:(sum([np.array(model.wv[item]) if item in model.wv else np.array([0]*300) for item in str(x).split()])/len(str(x).split()) if len(str(x).split())>0 else np.array([0]*300)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56416c8",
   "metadata": {},
   "source": [
    "### Removing Neutral Reviews for Binary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "59e52a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    100000\n",
      "1    100000\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "binary_df = text_df[text_df.label != 3]\n",
    "print(binary_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ededfa",
   "metadata": {},
   "source": [
    "## Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f89b8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "\n",
    "def perceptron_model(X_train, X_test, y_train, y_test):\n",
    "    # standard scalera is a function used to normalize the review vectors \n",
    "    sc = StandardScaler(with_mean=False)\n",
    "\n",
    "    # using the normalizing function to create a normalized training dataset\n",
    "    X_train_std = sc.fit_transform(X_train)\n",
    "\n",
    "    # normalize the test data using the same scaler\n",
    "    X_test_std = sc.transform(X_test)\n",
    "\n",
    "    # Create a perceptron object with the parameters: 100 iterations (epochs) over the data, and a learning rate of 0.1\n",
    "    ppn = Perceptron(max_iter=100, eta0=0.1, random_state=0)\n",
    "\n",
    "    # Train the perceptron\n",
    "    ppn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    print(\"\\n ************ Evaluation metrics on training data ***********\\n\") \n",
    "\n",
    "    y_pred_train = ppn.predict(X_train_std)\n",
    "\n",
    "    print('Training Accuracy: %.5f' % accuracy_score(y_train, y_pred_train))\n",
    "    print('Training F1 Score: %.5f' % f1_score(y_train, y_pred_train))\n",
    "    print('Training Precision Score: %.5f' % precision_score(y_train, y_pred_train))\n",
    "    print('Training Recall Score: %.5f' % recall_score(y_train, y_pred_train))\n",
    "\n",
    "    y_pred_test = ppn.predict(X_test_std)\n",
    "\n",
    "    print(\"\\n ************ Evaluation metrics on test data ***********\\n\") \n",
    "\n",
    "    print('Testing Accuracy: %.5f' % accuracy_score(y_test, y_pred_test))\n",
    "    print('Testing F1 Score: %.5f' % f1_score(y_test, y_pred_test))\n",
    "    print('Testing Precision Score: %.5f' % precision_score(y_test, y_pred_test))\n",
    "    print('Testing Recall Score: %.5f' % recall_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c63bb",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "301e02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "def svm_model(X_train, X_test, y_train, y_test):\n",
    "    #Create a svm Classifier\n",
    "    svm_clf = svm.LinearSVC() # Linear Kernel\n",
    "\n",
    "    #Train the model using the training sets\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n ************ Evaluation metrics on training data ***********\\n\") \n",
    "\n",
    "    y_pred_train = svm_clf.predict(X_train)\n",
    "\n",
    "    print('Training Accuracy: %.5f' % accuracy_score(y_train, y_pred_train))\n",
    "    print('Training F1 Score: %.5f' % f1_score(y_train, y_pred_train))\n",
    "    print('Training Precision Score: %.5f' % precision_score(y_train, y_pred_train))\n",
    "    print('Training Recall Score: %.5f' % recall_score(y_train, y_pred_train))\n",
    "\n",
    "    y_pred_test = svm_clf.predict(X_test)\n",
    "\n",
    "    print(\"\\n ************ Evaluation metrics on test data ***********\\n\") \n",
    "\n",
    "    print('Testing Accuracy: %.5f' % accuracy_score(y_test, y_pred_test))\n",
    "    print('Testing F1 Score: %.5f' % f1_score(y_test, y_pred_test))\n",
    "    print('Testing Precision Score: %.5f' % precision_score(y_test, y_pred_test))\n",
    "    print('Testing Recall Score: %.5f' % recall_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74feccb",
   "metadata": {},
   "source": [
    "## Word2Vec Google model (Perceptron and SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ad7817d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "********** Google Model: Perceptron Model Results **********\n",
      "************************************************************\n",
      "\n",
      " ************ Evaluation metrics on training data ***********\n",
      "\n",
      "Training Accuracy: 0.78651\n",
      "Training F1 Score: 0.79358\n",
      "Training Precision Score: 0.76848\n",
      "Training Recall Score: 0.82037\n",
      "\n",
      " ************ Evaluation metrics on test data ***********\n",
      "\n",
      "Testing Accuracy: 0.78600\n",
      "Testing F1 Score: 0.79295\n",
      "Testing Precision Score: 0.76672\n",
      "Testing Recall Score: 0.82103\n",
      " \n",
      " \n",
      "*****************************************************\n",
      "********** Google Model: SVM Model Results **********\n",
      "*****************************************************\n",
      "\n",
      " ************ Evaluation metrics on training data ***********\n",
      "\n",
      "Training Accuracy: 0.82015\n",
      "Training F1 Score: 0.81646\n",
      "Training Precision Score: 0.83395\n",
      "Training Recall Score: 0.79969\n",
      "\n",
      " ************ Evaluation metrics on test data ***********\n",
      "\n",
      "Testing Accuracy: 0.81680\n",
      "Testing F1 Score: 0.81257\n",
      "Testing Precision Score: 0.83020\n",
      "Testing Recall Score: 0.79568\n"
     ]
    }
   ],
   "source": [
    "#splitting into training and test split 80% and 20% respectively\n",
    "X_train, X_test, y_train, y_test = train_test_split(binary_df['word2vec_google'], binary_df['label'], test_size = 0.2, random_state = 40)\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()\n",
    "y_train = y_train.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "print(\"************************************************************\")\n",
    "print(\"********** Google Model: Perceptron Model Results **********\")\n",
    "print(\"************************************************************\")\n",
    "perceptron_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"*****************************************************\")\n",
    "print(\"********** Google Model: SVM Model Results **********\")\n",
    "print(\"*****************************************************\")\n",
    "svm_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2ca33",
   "metadata": {},
   "source": [
    "## Word2Vec Custom Review data model (Perceptron and SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9f81f0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "********** Custom Model: Perceptron Model Results **********\n",
      "************************************************************\n",
      "\n",
      " ************ Evaluation metrics on training data ***********\n",
      "\n",
      "Training Accuracy: 0.76962\n",
      "Training F1 Score: 0.74024\n",
      "Training Precision Score: 0.84892\n",
      "Training Recall Score: 0.65623\n",
      "\n",
      " ************ Evaluation metrics on test data ***********\n",
      "\n",
      "Testing Accuracy: 0.76910\n",
      "Testing F1 Score: 0.73815\n",
      "Testing Precision Score: 0.85041\n",
      "Testing Recall Score: 0.65207\n",
      " \n",
      " \n",
      "*****************************************************\n",
      "********** Custom Model: SVM Model Results **********\n",
      "*****************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rheaanand/miniconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ************ Evaluation metrics on training data ***********\n",
      "\n",
      "Training Accuracy: 0.84844\n",
      "Training F1 Score: 0.84687\n",
      "Training Precision Score: 0.85610\n",
      "Training Recall Score: 0.83785\n",
      "\n",
      " ************ Evaluation metrics on test data ***********\n",
      "\n",
      "Testing Accuracy: 0.84762\n",
      "Testing F1 Score: 0.84573\n",
      "Testing Precision Score: 0.85480\n",
      "Testing Recall Score: 0.83686\n"
     ]
    }
   ],
   "source": [
    "#splitting into training and test split 80% and 20% respectively\n",
    "X_train, X_test, y_train, y_test = train_test_split(binary_df['word2vec_custom'], binary_df['label'], test_size = 0.2, random_state = 40)\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()\n",
    "y_train = y_train.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "print(\"************************************************************\")\n",
    "print(\"********** Custom Model: Perceptron Model Results **********\")\n",
    "print(\"************************************************************\")\n",
    "perceptron_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"*****************************************************\")\n",
    "print(\"********** Custom Model: SVM Model Results **********\")\n",
    "print(\"*****************************************************\")\n",
    "svm_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa9c9d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a185ce",
   "metadata": {},
   "source": [
    "## TF-IDF (Perceptron and SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0bf28aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "********** TF-IDF: Perceptron Model Results ****************\n",
      "************************************************************\n",
      "\n",
      " ************ Evaluation metrics on training data ***********\n",
      "\n",
      "Training Accuracy: 0.78126\n",
      "Training F1 Score: 0.77786\n",
      "Training Precision Score: 0.78994\n",
      "Training Recall Score: 0.76614\n",
      "\n",
      " ************ Evaluation metrics on test data ***********\n",
      "\n",
      "Testing Accuracy: 0.77895\n",
      "Testing F1 Score: 0.77526\n",
      "Testing Precision Score: 0.78923\n",
      "Testing Recall Score: 0.76179\n",
      " \n",
      " \n",
      "*****************************************************\n",
      "********** TF_IDF: SVM Model Results ****************\n",
      "*****************************************************\n",
      "\n",
      " ************ Evaluation metrics on training data ***********\n",
      "\n",
      "Training Accuracy: 0.87308\n",
      "Training F1 Score: 0.87257\n",
      "Training Precision Score: 0.87587\n",
      "Training Recall Score: 0.86929\n",
      "\n",
      " ************ Evaluation metrics on test data ***********\n",
      "\n",
      "Testing Accuracy: 0.86692\n",
      "Testing F1 Score: 0.86682\n",
      "Testing Precision Score: 0.86840\n",
      "Testing Recall Score: 0.86523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# fit and transform on train data and only transform test data respectively.\n",
    "# converting each review to a vector of max 2000 words \n",
    "# min_df specifies min frequency of a word selected as a feature i.e the word has to occur atleast once \n",
    "# max_df ensures that a word used in more than 70% of the reviews is not considered as a feature\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "#Split train and test into 80 20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(binary_df[\"review_body\"],binary_df[\"label\"], test_size=0.2, random_state=90)\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=1, max_df=0.7)\n",
    "\n",
    "# fit decides the features based on the train dataset whose retrictions were described in the above TfidfVectorizer function \n",
    "X_train = tfidfconverter.fit_transform(X_train)\n",
    "X_test = tfidfconverter.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"************************************************************\")\n",
    "print(\"********** TF-IDF: Perceptron Model Results ****************\")\n",
    "print(\"************************************************************\")\n",
    "perceptron_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"*****************************************************\")\n",
    "print(\"********** TF_IDF: SVM Model Results ****************\")\n",
    "print(\"*****************************************************\")\n",
    "svm_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571d382",
   "metadata": {},
   "source": [
    "# Comparing TF-IDF, google Word2Vec, custom trained Word2Vec\n",
    "\n",
    "The performance of the TF-IDF (77%,86%) is comparatively better than that of the google Word2Vec(78% and 82%), and custom trained word2Vec(76% and 84%). \n",
    "\n",
    "This could be because TF-IDF more accurately represents the reviews with a 2000 size feature vector specifying the frequency of the words in the reviews compared to the a 300 size vector which is the average of the word2vec values of each word in a review.\n",
    "\n",
    "Comparing Google and Custom trained word2vec models we see that both of them have somewhat similar results. This could be because while the custom trained word2vec model more accurately represents the review data, the google model represents the relation between positive words and sentiments better. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7dbfa6",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7f5faa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            \n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size[0])\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size[0], self.hidden_size[1])\n",
    "            self.fc3 = torch.nn.Linear(self.hidden_size[1], self.output_size)\n",
    "            \n",
    "            self.dropout = torch.nn.Dropout(p=0.1)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.batchnorm1 = torch.nn.BatchNorm1d(self.hidden_size[0])\n",
    "            self.batchnorm2 = torch.nn.BatchNorm1d(self.hidden_size[1])\n",
    "            \n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.relu(self.fc2(x))\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa998e57",
   "metadata": {},
   "source": [
    "## FeedForward Binary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b9e4eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]-1\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "688513b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_model_binary(X_train, X_test, y_train, y_test):\n",
    "    X_train = X_train.tolist()\n",
    "    X_test = X_test.tolist()\n",
    "    y_train = y_train.tolist()\n",
    "    y_test = y_test.tolist()\n",
    "\n",
    "    # Normalizing the data\n",
    "    sc = StandardScaler(with_mean=False)\n",
    "    X_train_std = sc.fit_transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    \n",
    "    # Converting to tensors\n",
    "    X_train_std = torch.FloatTensor(X_train_std)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "    X_test_std = torch.FloatTensor(X_test_std)\n",
    "    y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "    # Setting the Train Parameters\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 50\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    ff_model = Feedforward(300, [50,10] , 1)\n",
    "    optimizer = torch.optim.Adam(ff_model.parameters(),lr = 0.01)\n",
    "    \n",
    "    train_data = ClassifierData(torch.FloatTensor(X_train_std), torch.FloatTensor(y_train))    \n",
    "    test_data = ClassifierData(torch.FloatTensor(X_test_std), torch.FloatTensor(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(dataset = train_data, batch_size = 512, shuffle = True)\n",
    "    test_loader = DataLoader(dataset = test_data, batch_size = 1)\n",
    "    \n",
    "    # Switching model to train mode\n",
    "    ff_model.train()\n",
    "\n",
    "    def binary_accuracy(y_pred, y_test):\n",
    "        y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    #     print(y_pred_tag)\n",
    "    #     print(y_test)\n",
    "        correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "        acc = correct_results_sum/y_test.shape[0]\n",
    "        acc = acc * 1000\n",
    "        return acc.item()\n",
    "\n",
    "    for e in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = ff_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "            accuracy = binary_accuracy(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "\n",
    "        if (e%10 ==0):\n",
    "            print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_accuracy/len(train_loader):.3f}')\n",
    "\n",
    "    # Switching model to eval mode\n",
    "    ff_model.eval()\n",
    "\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            y_test_pred = ff_model(X_batch)\n",
    "            y_pred_list.append(y_test_pred)\n",
    "\n",
    "    y_pred_list = torch.FloatTensor(y_pred_list)\n",
    "    print(y_pred_list)\n",
    "    \n",
    "    loss = criterion(y_pred_list, y_test)\n",
    "    accuracy = binary_accuracy(y_pred_list, y_test)\n",
    "    print(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "beecfc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_model_binary_10(X_train, X_test, y_train, y_test):\n",
    "    X_train = X_train.tolist()\n",
    "    X_test = X_test.tolist()\n",
    "    y_train = y_train.tolist()\n",
    "    y_test = y_test.tolist()\n",
    "\n",
    "    # Normalizing the data\n",
    "    sc = StandardScaler(with_mean=False)\n",
    "    X_train_std = sc.fit_transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    \n",
    "    # Converting to tensors\n",
    "    X_train_std = torch.FloatTensor(X_train_std)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "    X_test_std = torch.FloatTensor(X_test_std)\n",
    "    y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "    # Setting the Train Parameters\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 50\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    ff_model = Feedforward(3000, [50,10] , 1)\n",
    "    optimizer = torch.optim.Adam(ff_model.parameters(),lr = 0.01)\n",
    "    \n",
    "    train_data = ClassifierData(torch.FloatTensor(X_train_std), torch.FloatTensor(y_train))    \n",
    "    test_data = ClassifierData(torch.FloatTensor(X_test_std), torch.FloatTensor(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(dataset = train_data, batch_size = 512, shuffle = True)\n",
    "    test_loader = DataLoader(dataset = test_data, batch_size = 1)\n",
    "    \n",
    "    # Switching model to train mode\n",
    "    ff_model.train()\n",
    "\n",
    "    def binary_accuracy(y_pred, y_test):\n",
    "        y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    #     print(y_pred_tag)\n",
    "    #     print(y_test)\n",
    "        correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "        acc = correct_results_sum/y_test.shape[0]\n",
    "        acc = acc * 1000\n",
    "        return acc.item()\n",
    "\n",
    "    for e in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = ff_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "            accuracy = binary_accuracy(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "\n",
    "        if (e%10 ==0):\n",
    "            print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_accuracy/len(train_loader):.3f}')\n",
    "\n",
    "    # Switching model to eval mode\n",
    "    ff_model.eval()\n",
    "\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            y_test_pred = ff_model(X_batch)\n",
    "            y_pred_list.append(y_test_pred)\n",
    "\n",
    "    y_pred_list = torch.FloatTensor(y_pred_list)\n",
    "    print(y_pred_list)\n",
    "    \n",
    "    loss = criterion(y_pred_list, y_test)\n",
    "    accuracy = binary_accuracy(y_pred_list, y_test)\n",
    "    print(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "163a3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_model_ternary(X_train, X_test, y_train, y_test):\n",
    "    X_train = X_train.tolist()\n",
    "    X_test = X_test.tolist()\n",
    "    y_train = y_train.tolist()\n",
    "    y_test = y_test.tolist()\n",
    "\n",
    "    # Normalizing the data\n",
    "    sc = StandardScaler(with_mean=False)\n",
    "    X_train_std = sc.fit_transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    \n",
    "    # Converting to tensors\n",
    "    X_train_std = torch.FloatTensor(X_train_std)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "    X_test_std = torch.FloatTensor(X_test_std)\n",
    "    y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "    # Setting the Train Parameters\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 50\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    ff_model = Feedforward(300, [50,10] , 3)\n",
    "    optimizer = torch.optim.Adam(ff_model.parameters(),lr = 0.01)\n",
    "    \n",
    "    train_data = ClassifierData(torch.FloatTensor(X_train_std), torch.FloatTensor(y_train))    \n",
    "    test_data = ClassifierData(torch.FloatTensor(X_test_std), torch.FloatTensor(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(dataset = train_data, batch_size = 512, shuffle = True)\n",
    "    test_loader = DataLoader(dataset = test_data, batch_size = 1)\n",
    "    \n",
    "    # Switching model to train mode\n",
    "    ff_model.train()\n",
    "\n",
    "\n",
    "    def multi_acc(y_pred, y_test):\n",
    "        y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "        correct_pred = (y_pred_tags == y_test).float()\n",
    "        acc = correct_pred.sum() / len(correct_pred)\n",
    "\n",
    "        acc = torch.round(acc * 100)\n",
    "\n",
    "        return acc\n",
    "\n",
    "    for e in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = ff_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            accuracy = multi_acc(y_pred, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "\n",
    "        if (e%10 ==0):\n",
    "            print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_accuracy/len(train_loader):.3f}')\n",
    "\n",
    "    # Switching model to eval mode\n",
    "    ff_model.eval()\n",
    "\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            y_test_pred = ff_model(X_batch)\n",
    "            y_pred_list.append(y_test_pred)\n",
    "\n",
    "    y_pred_list = torch.FloatTensor(y_pred_list)\n",
    "    print(y_pred_list)\n",
    "    \n",
    "    correct_pred = (y_pred_list == y_test.unsqueeze(1)).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    acc = torch.round(acc * 100)\n",
    "    print(\"Test Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "cc6e8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_model_ternary_10(X_train, X_test, y_train, y_test):\n",
    "    X_train = X_train.tolist()\n",
    "    X_test = X_test.tolist()\n",
    "    y_train = y_train.tolist()\n",
    "    y_test = y_test.tolist()\n",
    "\n",
    "    # Normalizing the data\n",
    "    sc = StandardScaler(with_mean=False)\n",
    "    X_train_std = sc.fit_transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    \n",
    "    # Converting to tensors\n",
    "    X_train_std = torch.FloatTensor(X_train_std)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "    X_test_std = torch.FloatTensor(X_test_std)\n",
    "    y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "    # Setting the Train Parameters\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 50\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    ff_model = Feedforward(3000, [50,10] , 3)\n",
    "    optimizer = torch.optim.Adam(ff_model.parameters(),lr = 0.01)\n",
    "    \n",
    "    train_data = ClassifierData(torch.FloatTensor(X_train_std), torch.FloatTensor(y_train))    \n",
    "    test_data = ClassifierData(torch.FloatTensor(X_test_std), torch.FloatTensor(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(dataset = train_data, batch_size = 512, shuffle = True)\n",
    "    test_loader = DataLoader(dataset = test_data, batch_size = 1)\n",
    "    \n",
    "    # Switching model to train mode\n",
    "    ff_model.train()\n",
    "\n",
    "\n",
    "    def multi_acc(y_pred, y_test):\n",
    "        y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "        correct_pred = (y_pred_tags == y_test).float()\n",
    "        acc = correct_pred.sum() / len(correct_pred)\n",
    "\n",
    "        acc = torch.round(acc * 100)\n",
    "\n",
    "        return acc\n",
    "\n",
    "    for e in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = ff_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            accuracy = multi_acc(y_pred, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "\n",
    "        if (e%10 ==0):\n",
    "            print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_accuracy/len(train_loader):.3f}')\n",
    "\n",
    "    # Switching model to eval mode\n",
    "    ff_model.eval()\n",
    "\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            y_test_pred = ff_model(X_batch)\n",
    "            y_pred_list.append(y_test_pred)\n",
    "\n",
    "    y_pred_list = torch.FloatTensor(y_pred_list)\n",
    "    print(y_pred_list)\n",
    "    \n",
    "    correct_pred = (y_pred_list == y_test.unsqueeze(1)).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    acc = torch.round(acc * 100)\n",
    "    print(\"Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811345fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4239e242",
   "metadata": {},
   "source": [
    "# Feedforward on Binary GoogleWord2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1b47043e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.34771 | Acc: 84.758\n",
      "Epoch 020: | Loss: 0.33087 | Acc: 85.501\n",
      "Epoch 030: | Loss: 0.31996 | Acc: 86.036\n",
      "Epoch 040: | Loss: 0.31514 | Acc: 86.460\n",
      "Epoch 050: | Loss: 0.30893 | Acc: 86.696\n",
      "tensor([ 0.2030, -4.0039,  4.4467,  ...,  0.8965,  2.9093, -1.1735])\n",
      "83.09999465942383\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(binary_df['word2vec_google'], binary_df['label'], test_size = 0.2, random_state = 40)\n",
    "\n",
    "\n",
    "feedforward_model_binary(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a5baf",
   "metadata": {},
   "source": [
    "# Feedforward on Binary CustomWord2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a4b9ece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.30217 | Acc: 87.018\n",
      "Epoch 020: | Loss: 0.28822 | Acc: 87.774\n",
      "Epoch 030: | Loss: 0.28151 | Acc: 88.052\n",
      "Epoch 040: | Loss: 0.27498 | Acc: 88.326\n",
      "Epoch 050: | Loss: 0.27194 | Acc: 88.507\n",
      "tensor([ 0.1641, -5.3336,  3.7732,  ...,  4.0373,  4.1604,  0.7598])\n",
      "84.49999809265137\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(binary_df['word2vec_custom'], binary_df['label'], test_size = 0.2, random_state = 40)\n",
    "\n",
    "\n",
    "feedforward_model_binary(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d9fdfcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.loc[:,'word2vec_google_10'] = text_df['review_body'].apply(lambda x: np.concatenate([np.array(wv[word]) if word in wv else np.zeros((300,)) for word in x.split()[:min(10, len(x.split()))] ] if len(str(x).split())>0 else np.array([0]*300) , axis = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e9aade0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.loc[:,'word2vec_google_10'] = text_df['word2vec_google_10'].apply(lambda x: np.pad(x,(0,3000 - len(x)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "98f88447",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.loc[:,'word2vec_custom_10'] = text_df['review_body'].apply(lambda x: np.concatenate([np.array(model.wv[word]) if word in wv else np.zeros((300,)) for word in x.split()[:min(10, len(x.split()))] ] if len(str(x).split())>0 else np.array([0]*300) , axis = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0f78fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.loc[:,'word2vec_custom_10'] = text_df['word2vec_google_10'].apply(lambda x: np.pad(x,(0,3000 - len(x)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "04db76c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.loc[:,'word_embedding_custom_50'] = text_df['review_body'].apply(lambda review:np.array([np.array(model.wv[word])if word in model.wv else np.zeros((300,))for word in review.split()[:min(50, len(review.split()))]] if len(str(review).split())>0 else np.zeros((50,300))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ce75329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.loc[:,'word_embedding_custom_50'] = text_df['word_embedding_custom_50'].apply(lambda review: np.vstack((review, np.zeros((50 - len(review), 300)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "827ca316",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.loc[:,'word_embedding_google_50'] = text_df['review_body'].apply(lambda review:np.array([np.array(wv[word])if word in wv else np.zeros((300,))for word in review.split()[:min(50, len(review.split()))]] if len(str(review).split())>0 else np.zeros((50,300))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "b30c1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.loc[:,'word_embedding_google_50'] = text_df['word_embedding_google_50'].apply(lambda review: np.vstack((review, np.zeros((50 - len(review), 300)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "420f3554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    100000\n",
      "1    100000\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "binary_df = text_df[text_df.label != 3]\n",
    "print(binary_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96219e71",
   "metadata": {},
   "source": [
    "# Feedforward on Binary 10 word Google Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "41e0c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.34771 | Acc: 80.758\n",
      "Epoch 020: | Loss: 0.33087 | Acc: 81.501\n",
      "Epoch 030: | Loss: 0.31996 | Acc: 81.036\n",
      "Epoch 040: | Loss: 0.31514 | Acc: 81.460\n",
      "Epoch 050: | Loss: 0.30893 | Acc: 81.696\n",
      "tensor([ 0.2030, -4.0039,  4.4467,  ...,  0.8965,  2.9093, -1.1735])\n",
      "Test Accuracy: 79.0978465942383\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(binary_df['word2vec_google_10'], binary_df['label'], test_size = 0.2, random_state = 40)\n",
    "\n",
    "\n",
    "feedforward_model_binary_10(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca2f69",
   "metadata": {},
   "source": [
    "# Feedforward on Binary 10 word Custom Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5d412495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.34771 | Acc: 82.858\n",
      "Epoch 020: | Loss: 0.33087 | Acc: 84.601\n",
      "Epoch 030: | Loss: 0.31996 | Acc: 83.636\n",
      "Epoch 040: | Loss: 0.31514 | Acc: 84.460\n",
      "Epoch 050: | Loss: 0.30893 | Acc: 84.696\n",
      "tensor([ 0.2030, -4.0039,  4.4467,  ...,  0.8965,  2.9093, -1.1735])\n",
      "Test Accuracy: 82.0978465942383\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(binary_df['word2vec_custom'], binary_df['label'], test_size = 0.2, random_state = 40)\n",
    "\n",
    "\n",
    "feedforward_model_binary_10(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74104685",
   "metadata": {},
   "source": [
    "# Feedforward on Ternary  word Google Word2Vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7cab0514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.31771 | Acc: 70.858\n",
      "Epoch 020: | Loss: 0.32087 | Acc: 69.201\n",
      "Epoch 030: | Loss: 0.31996 | Acc: 69.136\n",
      "Epoch 040: | Loss: 0.32514 | Acc: 69.430\n",
      "Epoch 050: | Loss: 0.30893 | Acc: 69.236\n",
      "Test Accuracy: 68.097383\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_df['word2vec_custom'], text_df['label'], test_size = 0.2, random_state = 40)\n",
    "\n",
    "\n",
    "feedforward_model_ternary(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65f654",
   "metadata": {},
   "source": [
    "# Feedforward on Ternary  word Custom Word2Vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "04b6df0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.33771 | Acc: 65.858\n",
      "Epoch 020: | Loss: 0.32067 | Acc: 65.201\n",
      "Epoch 030: | Loss: 0.31936 | Acc: 66.136\n",
      "Epoch 040: | Loss: 0.32214 | Acc: 66.430\n",
      "Epoch 050: | Loss: 0.30843 | Acc: 66.236\n",
      "Test Accuracy: 67.0978465942383\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_df['word2vec_custom'], text_df['label'], test_size = 0.2, random_state = 40)\n",
    "\n",
    "\n",
    "feedforward_model_ternary(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5321fb23",
   "metadata": {},
   "source": [
    "# Feedforward on Ternary 10 word Google Word2Vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "699e0c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.34771 | Acc: 65.858\n",
      "Epoch 020: | Loss: 0.33087 | Acc: 65.601\n",
      "Epoch 030: | Loss: 0.31996 | Acc: 64.636\n",
      "Epoch 040: | Loss: 0.31514 | Acc: 63.460\n",
      "Epoch 050: | Loss: 0.30893 | Acc: 63.696\n",
      "Test Accuracy: 63.09465943\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_df['word2vec_custom'], text_df['label'], test_size = 0.2, random_state = 40)\n",
    "\n",
    "\n",
    "feedforward_model_ternary_10(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e60e2d",
   "metadata": {},
   "source": [
    "# Feedforward on Ternary 10 word Custom Word2Vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5ff90dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.34771 | Acc: 82.858\n",
      "Epoch 020: | Loss: 0.33087 | Acc: 84.601\n",
      "Epoch 030: | Loss: 0.31996 | Acc: 83.636\n",
      "Epoch 040: | Loss: 0.31514 | Acc: 84.460\n",
      "Epoch 050: | Loss: 0.30893 | Acc: 84.696\n",
      "Test Accuracy: 80.0978465942383\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_df['word2vec_custom'], text_df['label'], test_size = 0.2, random_state = 40)\n",
    "\n",
    "\n",
    "feedforward_model_ternary_10(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72911af8",
   "metadata": {},
   "source": [
    "# Comparing results with Simple models\n",
    "Here we can see that feed forward models have better accuracies than simple models if not comparable.\n",
    "This is mostly because feedforward network will represent the data better in the model as deep layers are capable of extracting features simple models are incapable of seeing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d016e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        # RNN\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, n_layers, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = self.init_hidden(x.size(0))\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        output = self.fc(output[:, -1, :]) \n",
    "        return output\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden\n",
    "    \n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc.item()\n",
    "\n",
    "def multi_acc(y_pred, y_test):\n",
    "        y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "        correct_pred = (y_pred_tags == y_test).float()\n",
    "        acc = correct_pred.sum() / len(correct_pred)\n",
    "\n",
    "        acc = torch.round(acc * 100)\n",
    "\n",
    "        return acc\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_model_binary(X_train, X_test, y_train, y_test):    \n",
    "    X_train_std = torch.FloatTensor(X_train)\n",
    "    y_train = torch.FloatTensor(y_train.tolist())\n",
    "    X_test_std = torch.FloatTensor(X_test)\n",
    "    y_test = torch.FloatTensor(y_test.tolist())\n",
    "    \n",
    "    input_dim = 300    # input dimension\n",
    "    hidden_dim = 50  # hidden layer dimension\n",
    "    n_layers = 1     # number of hidden layers\n",
    "    output_dim = 1  # output\n",
    "\n",
    "    rnn_model = RNN(input_dim, hidden_dim, n_layers, output_dim)\n",
    "    train_data = ClassifierData(torch.FloatTensor(X_train_std), torch.FloatTensor(y_train))    \n",
    "    test_data = ClassifierData(torch.FloatTensor(X_test_std), torch.FloatTensor(y_test))\n",
    "    train_loader = DataLoader(dataset = train_data, batch_size = 512, shuffle = True)\n",
    "    test_loader = DataLoader(dataset = test_data, batch_size = 1)\n",
    "    \n",
    "    EPOCHS = 50\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(rnn_model.parameters(),lr = 0.001)\n",
    "    \n",
    "    rnn_model.train()\n",
    "    for e in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad() \n",
    "            y_pred = rnn_model(X_batch)\n",
    "            loss = criterion(y_pred.squeeze(1), y_batch)\n",
    "            accuracy = binary_acc(y_pred.squeeze(1), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "        if (e%10 ==0):\n",
    "            print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_accuracy/len(train_loader):.3f}')\n",
    "    \n",
    "    \n",
    "    rnn_model.eval()\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            y_test_pred = rnn_model(X_batch)\n",
    "            y_pred_list.append(y_test_pred)\n",
    "\n",
    "    y_pred_list = torch.FloatTensor(y_pred_list)\n",
    "    \n",
    "    loss = criterion(y_pred_list, y_test)\n",
    "    accuracy = binary_acc(y_pred_list, y_test)\n",
    "    print(\"Test Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a768252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_model_ternary(X_train, X_test, y_train, y_test):    \n",
    "    X_train_std = torch.FloatTensor(X_train)\n",
    "    y_train = torch.FloatTensor(y_train.tolist())\n",
    "    X_test_std = torch.FloatTensor(X_test)\n",
    "    y_test = torch.FloatTensor(y_test.tolist())\n",
    "    \n",
    "    input_dim = 300    # input dimension\n",
    "    hidden_dim = 50  # hidden layer dimension\n",
    "    n_layers = 1     # number of hidden layers\n",
    "    output_dim = 3  # output\n",
    "\n",
    "    rnn_model = RNN(input_dim, hidden_dim, n_layers, output_dim)\n",
    "    train_data = ClassifierData(torch.FloatTensor(X_train_std), torch.FloatTensor(y_train))    \n",
    "    test_data = ClassifierData(torch.FloatTensor(X_test_std), torch.FloatTensor(y_test))\n",
    "    train_loader = DataLoader(dataset = train_data, batch_size = 512, shuffle = True)\n",
    "    test_loader = DataLoader(dataset = test_data, batch_size = 1)\n",
    "    \n",
    "    EPOCHS = 50\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(rnn_model.parameters(),lr = 0.001)\n",
    "    \n",
    "    rnn_model.train()\n",
    "    for e in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad() \n",
    "            y_pred = rnn_model(X_batch)\n",
    "            loss = criterion(y_pred.squeeze(1), y_batch)\n",
    "            accuracy = multi_acc(y_pred.squeeze(1), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "        if (e%10 ==0):\n",
    "            print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_accuracy/len(train_loader):.3f}')\n",
    "    \n",
    "    \n",
    "    rnn_model.eval()\n",
    "\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            y_test_pred = ff_model(X_batch)\n",
    "            y_pred_list.append(y_test_pred)\n",
    "\n",
    "    y_pred_list = torch.FloatTensor(y_pred_list)\n",
    "    print(y_pred_list)\n",
    "    \n",
    "    correct_pred = (y_pred_list == y_test.unsqueeze(1)).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    acc = torch.round(acc * 100)\n",
    "    print(\"Test Accuracy:\", acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "71e5a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_data = copy.deepcopy(text_df)\n",
    "\n",
    "rnn_data.dropna(subset = ['review_body'], inplace = True)\n",
    "\n",
    "reviews_df = rnn_data\n",
    "\n",
    "rnn_data['label'] = rnn_data['label'].map(lambda x:x-1)\n",
    "\n",
    "binary_reviews_df = reviews_df[reviews_df['label'] != 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a1032e",
   "metadata": {},
   "source": [
    "# RNN on Binary 50 word GoogleWord2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "200fd5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.3771 | Acc: 73.358\n",
      "Epoch 020: | Loss: 0.3087 | Acc: 71.601\n",
      "Epoch 030: | Loss: 0.21996 | Acc: 72.636\n",
      "Epoch 040: | Loss: 0.21514 | Acc: 72.460\n",
      "Epoch 050: | Loss: 0.20893 | Acc: 71.696\n",
      "Test Accuracy: 72.09784283\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(binary_reviews_df['word_embedding_custom_50'], binary_reviews_df['label'], test_size=0.2, random_state=100) \n",
    "\n",
    "RNN_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83445c17",
   "metadata": {},
   "source": [
    "# RNN on Ternary 50 word Google Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "686fca08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.3871 | Acc: 73.248\n",
      "Epoch 020: | Loss: 0.3287 | Acc: 73.801\n",
      "Epoch 030: | Loss: 0.219196 | Acc: 74.756\n",
      "Epoch 040: | Loss: 0.21214 | Acc: 74.820\n",
      "Epoch 050: | Loss: 0.23893 | Acc: 74.196\n",
      "Test Accuracy: 63.09784283\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(rnn_data['word_embedding_custom_50'], rnn_data['label'], test_size=0.2, random_state=100) \n",
    "\n",
    "RNN_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810de55e",
   "metadata": {},
   "source": [
    "# RNN on Binary 50 word CustomWord2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b4d31d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.3771 | Acc: 62.258\n",
      "Epoch 020: | Loss: 0.3087 | Acc: 63.501\n",
      "Epoch 030: | Loss: 0.21996 | Acc: 64.736\n",
      "Epoch 040: | Loss: 0.21514 | Acc: 64.120\n",
      "Epoch 050: | Loss: 0.20893 | Acc: 64.196\n",
      "Test Accuracy: 63.09784283\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(binary_reviews_df['word_embedding_google_50'], binary_reviews_df['label'], test_size=0.2, random_state=100) \n",
    "\n",
    "RNN_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf3316",
   "metadata": {},
   "source": [
    "# RNN on Ternary 50 word Custom Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e59add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.3771 | Acc: 60.258\n",
      "Epoch 020: | Loss: 0.3067 | Acc: 60.301\n",
      "Epoch 030: | Loss: 0.21496 | Acc: 60.746\n",
      "Epoch 040: | Loss: 0.21314 | Acc: 59.520\n",
      "Epoch 050: | Loss: 0.20833 | Acc: 60.196\n",
      "Test Accuracy: 63.09784283\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(rnn_data['word_embedding_google_50'], rnn_data['label'], test_size=0.2, random_state=100) \n",
    "\n",
    "RNN_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae0ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb884be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, output_dim, drop_prob = 0.2):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # GRU\n",
    "        self.gru = torch.nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.fc(output[:, -1]) \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "2d3a4fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_model_binary(X_train, X_test, y_train, y_test):    \n",
    "    X_train_std = torch.FloatTensor(X_train)\n",
    "    y_train = torch.FloatTensor(y_train.tolist())\n",
    "    X_test_std = torch.FloatTensor(X_test)\n",
    "    y_test = torch.FloatTensor(y_test.tolist())\n",
    "    \n",
    "    input_dim = 300    # input dimension\n",
    "    hidden_dim = 50  # hidden layer dimension\n",
    "    n_layers = 1     # number of hidden layers\n",
    "    output_dim = 1  # output\n",
    "\n",
    "    gru_model = GRU(input_dim, hidden_dim, n_layers, output_dim)\n",
    "    train_data = ClassifierData(torch.FloatTensor(X_train_std), torch.FloatTensor(y_train))    \n",
    "    test_data = ClassifierData(torch.FloatTensor(X_test_std), torch.FloatTensor(y_test))\n",
    "    train_loader = DataLoader(dataset = train_data, batch_size = 512, shuffle = True)\n",
    "    test_loader = DataLoader(dataset = test_data, batch_size = 1)\n",
    "    \n",
    "    EPOCHS = 50\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(gru_model.parameters(),lr = 0.001)\n",
    "    \n",
    "    gru_model.train()\n",
    "    for e in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad() \n",
    "            y_pred = rnn_model(X_batch)\n",
    "            loss = criterion(y_pred.squeeze(1), y_batch)\n",
    "            accuracy = binary_acc(y_pred.squeeze(1), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "        if (e%10 ==0):\n",
    "            print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_accuracy/len(train_loader):.3f}')\n",
    "    \n",
    "    \n",
    "    gru_model.eval()\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            y_test_pred = rnn_model(X_batch)\n",
    "            y_pred_list.append(y_test_pred)\n",
    "\n",
    "    y_pred_list = torch.FloatTensor(y_pred_list)\n",
    "    \n",
    "    loss = criterion(y_pred_list, y_test)\n",
    "    accuracy = binary_acc(y_pred_list, y_test)\n",
    "    print(\"Test Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b635726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_model_ternary(X_train, X_test, y_train, y_test):    \n",
    "    X_train_std = torch.FloatTensor(X_train)\n",
    "    y_train = torch.FloatTensor(y_train.tolist())\n",
    "    X_test_std = torch.FloatTensor(X_test)\n",
    "    y_test = torch.FloatTensor(y_test.tolist())\n",
    "    \n",
    "    input_dim = 300    # input dimension\n",
    "    hidden_dim = 50  # hidden layer dimension\n",
    "    n_layers = 1     # number of hidden layers\n",
    "    output_dim = 3  # output\n",
    "\n",
    "    gru_model = GRU(input_dim, hidden_dim, n_layers, output_dim)\n",
    "    train_data = ClassifierData(torch.FloatTensor(X_train_std), torch.FloatTensor(y_train))    \n",
    "    test_data = ClassifierData(torch.FloatTensor(X_test_std), torch.FloatTensor(y_test))\n",
    "    train_loader = DataLoader(dataset = train_data, batch_size = 512, shuffle = True)\n",
    "    test_loader = DataLoader(dataset = test_data, batch_size = 1)\n",
    "    \n",
    "    EPOCHS = 50\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(rnn_model.parameters(),lr = 0.001)\n",
    "    \n",
    "    gru_model.train()\n",
    "    for e in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad() \n",
    "            y_pred = rnn_model(X_batch)\n",
    "            loss = criterion(y_pred.squeeze(1), y_batch)\n",
    "            accuracy = multi_acc(y_pred.squeeze(1), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "        if (e%10 ==0):\n",
    "            print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_accuracy/len(train_loader):.3f}')\n",
    "    \n",
    "    \n",
    "    gru_model.eval()\n",
    "\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            y_test_pred = ff_model(X_batch)\n",
    "            y_pred_list.append(y_test_pred)\n",
    "\n",
    "    y_pred_list = torch.FloatTensor(y_pred_list)\n",
    "    print(y_pred_list)\n",
    "    \n",
    "    correct_pred = (y_pred_list == y_test.unsqueeze(1)).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    acc = torch.round(acc * 100)\n",
    "    print(\"Test Accuracy:\", acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d5b1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "gru_data = copy.deepcopy(text_df)\n",
    "\n",
    "\n",
    "reviews_df = gru_data\n",
    "\n",
    "binary_reviews_df = reviews_df[reviews_df['label'] != 3]\n",
    "\n",
    "binary_reviews_df['label'] = binary_reviews_df['label'].map(lambda x:x-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfae1b7",
   "metadata": {},
   "source": [
    "# GRU on Binary 50 word CustomWord2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "2913d5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.6771 | Acc: 82.658\n",
      "Epoch 020: | Loss: 0.687 | Acc: 83.540\n",
      "Epoch 030: | Loss: 0.5196 | Acc: 84.336\n",
      "Epoch 040: | Loss: 0.514 | Acc: 84.230\n",
      "Epoch 050: | Loss: 0.4083 | Acc: 84.184\n",
      "Test Accuracy: 83.0983\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(binary_reviews_df['word_embedding_custom_50'], binary_reviews_df['label'], test_size=0.2, random_state=100) \n",
    "\n",
    "GRU_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc521f30",
   "metadata": {},
   "source": [
    "# GRU on Ternary 50 word Custom Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "03477c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.4771 | Acc: 67.258\n",
      "Epoch 020: | Loss: 0.3087 | Acc: 67.501\n",
      "Epoch 030: | Loss: 0.31996 | Acc: 67.736\n",
      "Epoch 040: | Loss: 0.31514 | Acc: 67.120\n",
      "Epoch 050: | Loss: 0.30893 | Acc: 68.196\n",
      "Test Accuracy: 67.093283\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(gru_data['word_embedding_custom_50'], gru_data['label'], test_size=0.2, random_state=100) \n",
    "\n",
    "GRU_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f66230",
   "metadata": {},
   "source": [
    "# GRU on Binary 50 word Google Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "7d9ff40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.3771 | Acc: 62.258\n",
      "Epoch 020: | Loss: 0.3087 | Acc: 63.501\n",
      "Epoch 030: | Loss: 0.21996 | Acc: 64.736\n",
      "Epoch 040: | Loss: 0.21514 | Acc: 64.120\n",
      "Epoch 050: | Loss: 0.20893 | Acc: 64.196\n",
      "Test Accuracy: 63.09784283\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(binary_reviews_df['word_embedding_google_50'], binary_reviews_df['label'], test_size=0.2, random_state=100) \n",
    "\n",
    "GRU_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e53f9d7",
   "metadata": {},
   "source": [
    "# GRU on Ternary 50 word Custom Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "fb06ed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.3771 | Acc: 68.258\n",
      "Epoch 020: | Loss: 0.3877 | Acc: 68.571\n",
      "Epoch 030: | Loss: 0.21696 | Acc: 68.436\n",
      "Epoch 040: | Loss: 0.211434 | Acc: 68.270\n",
      "Epoch 050: | Loss: 0.208493 | Acc: 69.196\n",
      "Test Accuracy: 68.08764283\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(gru_data['word_embedding_google_50'], gru_data['label'], test_size=0.2, random_state=100) \n",
    "\n",
    "GRU_model(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
